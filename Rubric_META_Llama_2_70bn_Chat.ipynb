{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barbaroja2000/rubric/blob/main/Rubric_META_Llama_2_70bn_Chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avy-BGZ_LVMd"
      },
      "source": [
        "#Evaluating Llama-2 70bn Chat against a custom Rubrik with GPT-4\n",
        "\n",
        "Uses:\n",
        "\n",
        "*   Langsmith for custom rubrik datasets & Evaluation Framework\n",
        "*   GPT-4 To evaluate LLM output\n",
        "*   Llama-2 chat on HuggingFace Hub\n",
        "\n",
        "---\n",
        "\n",
        "Test:\n",
        "\n",
        "*  provides a quality assesment 0-5 and a rationalle for the score\n",
        "* The interactive table can be used to Check the model output against the GPT-4 assesment\n",
        "* Possible improvment here is to be able to manually update the score returned from the model\n",
        "\n",
        "---\n",
        "\n",
        "Model Llama-2 70bn Chat:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Refs:\n",
        "\n",
        "https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\n",
        "\n",
        "https://python.langchain.com/docs/langsmith/\n",
        "\n",
        "---\n",
        "\n",
        "Requires:\n",
        "*   Test dataset in Langsmith: This notebook shows how to set this up https://github.com/barbaroja2000/rubric/blob/main/Langsmith_Rubric_Dataset_Creator.ipynb\n",
        "*   HuggingFace API Key\n",
        "*   Langchain Key\n",
        "*   OpenAI Key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQcugdHORym3"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langsmith openai tiktoken huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jaFdeZ7R937"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "import requests\n",
        "\n",
        "os.environ['LANGCHAIN_ENDPOINT']= \"https://api.smith.langchain.com\"\n",
        "os.environ['LANGCHAIN_API_KEY']= userdata.get('langchain_api_key')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('huggingface_api_key')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiLLCXGpls09"
      },
      "outputs": [],
      "source": [
        "project_name  = \"META Llama-2 70bn Chat\"\n",
        "rubric_model = \"meta-llama/Llama-2-70b-chat-hf\"\n",
        "rubric_temperature=0\n",
        "evaluator_model=\"gpt-4-1106-preview\"\n",
        "evaluator_temperature=0\n",
        "dataset_name = \"General Rubric: Extended\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etphUwWeNYyS"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFaceHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92CBWi9KNaS6",
        "outputId": "41cec6b7-fb70-45c1-96bc-9a76d027fe47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "llm = HuggingFaceHub(repo_id=rubric_model, model_kwargs={\"temperature\": 0.1, \"max_length\": 4000})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_iBOXoJnMdx"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "client = Client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSoozydgbuBU"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate\n",
        "\n",
        "# create a string template for a System role\n",
        "system_template = \"\"\"\n",
        "Please provide a detailed, accurate, and contextually relevant response to the following query.\n",
        "Your answer should demonstrate a clear understanding of the subject matter, considering any logical,\n",
        "ethical, historical, or cultural aspects involved. Ensure the response is tailored to the specific\n",
        "requirements of the question, whether it involves analysis, explanation, creativity, or problem-solving.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2CCDSmzUoEa"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableMap, RunnablePassthrough\n",
        "\n",
        "\n",
        "def create_runnable():\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "      (\"system\", system_template),\n",
        "      (\"human\", \"{input}\")\n",
        "    ])\n",
        "    return RunnableMap({\"input\": RunnablePassthrough()}) | prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMLmau0XXJXp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class QualityEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based quality  evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=evaluator_model, temperature=evaluator_temperature)\n",
        "\n",
        "        template = \"\"\" Evaluate the answer provided from 0 to 5, with 5 being the best quality answer, and 0 where the answer is false or not provided\n",
        "        --------\n",
        "        QUESTION: {input}\n",
        "        --------\n",
        "        ANSWER: {prediction}\n",
        "        --------\n",
        "        Consisely reason step by step about why the given score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = LLMChain.from_string(llm=llm, template=template)\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"quality\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain(\n",
        "            dict(input=input, prediction=prediction), **kwargs\n",
        "        )\n",
        "        lines = evaluator_result[\"text\"].strip().split(\"\\n\")\n",
        "        reasoning = evaluator_result[\"text\"].strip()\n",
        "        score = lines[-1]\n",
        "        if score is not None:\n",
        "            score = float(score.strip())\n",
        "        return {\"score\": score, \"reasoning\": reasoning}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeUosEoouWxV"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "def rando():\n",
        "  # Generating a random 4-letter string\n",
        "  random_string = ''.join(random.choices(string.ascii_letters, k=4))\n",
        "  return random_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuOccmtlVEg-",
        "outputId": "7b9ae702-ade3-45fa-b1e5-866ccb241bfe"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'META Llama-2 70bn Chat-HCat' at:\n",
            "https://smith.langchain.com/o/8bebdbff-9433-40f6-b7e1-f013521a8100/datasets/dfedd53c-204a-4b69-9f64-51a61568a7fd/compare?selectedSessions=a781f171-4134-404f-bc0d-45f7bf487d01\n",
            "\n",
            "View all tests for Dataset General Rubric: Extended at:\n",
            "https://smith.langchain.com/o/8bebdbff-9433-40f6-b7e1-f013521a8100/datasets/dfedd53c-204a-4b69-9f64-51a61568a7fd\n",
            "[------>                                           ] 16/123"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2426)')': /feedback\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[------------------------------------->            ] 93/123"
          ]
        }
      ],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "evaluation_config = RunEvalConfig(\n",
        "    input_key=\"question\",\n",
        "    output_key=\"output\",\n",
        "    custom_evaluators = [QualityEvaluator()],\n",
        ")\n",
        "\n",
        "project_metadata = {\n",
        "  rubric_model : rubric_model,\n",
        "  rubric_temperature : rubric_temperature,\n",
        "  evaluator_model : evaluator_model,\n",
        "  evaluator_temperature: evaluator_temperature\n",
        "}\n",
        "\n",
        "run = run_on_dataset(\n",
        "    client=client,\n",
        "    project_metadata = project_metadata,\n",
        "    project_name = f\"{project_name}-{rando()}\",\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=create_runnable,\n",
        "    evaluation=evaluation_config,\n",
        "    concurrency_level=1, #This will bork if not provided or set to anything other than 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-X_UI8qrCqc"
      },
      "outputs": [],
      "source": [
        "# Extracting the required information\n",
        "extracted_data = []\n",
        "for key, value in run['results'].items():\n",
        "    output = value[\"output\"] if \"output\" in value else None\n",
        "    question = value['input']['question']\n",
        "    for feedback_item in value['feedback']:\n",
        "        if feedback_item.key == 'quality':\n",
        "            quality = feedback_item.score\n",
        "            feedback = feedback_item.comment\n",
        "            extracted_data.append({\n",
        "                'question': question,\n",
        "                'output': output,\n",
        "                'quality': quality,\n",
        "                'feedback': feedback\n",
        "            })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5Ud9sKzsolt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "df = pd.DataFrame(extracted_data)\n",
        "\n",
        "data_table.DataTable(df, include_index=False, num_rows_per_page=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JSLMa0uY2M6",
        "outputId": "af5c9ca7-5fea-46a4-e35d-5dec5b4712a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "pd.to_numeric(df.quality).mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = list(client.list_examples(dataset_name=dataset_name))"
      ],
      "metadata": {
        "id": "bRvcNKD3YMrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new DataFrame from the list\n",
        "list_df = pd.DataFrame([{'question': item.inputs['question'], 'category': item.inputs['category']} for item in examples])"
      ],
      "metadata": {
        "id": "ToxemuWAqLEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the DataFrames\n",
        "merged_df = pd.merge(df, list_df, on='question')"
      ],
      "metadata": {
        "id": "Uv_rChDXqO8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'category' and calculate the mean of 'quality' scores\n",
        "avg_scores_per_category = merged_df.groupby('category')['quality'].mean()\n",
        "\n",
        "print(avg_scores_per_category)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHFQPzvlqYhl",
        "outputId": "2ba39da9-7460-4116-8500-83e041558e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "category\n",
            "Categorization                           4.363636\n",
            "Coding                                   2.384615\n",
            "Creative Writing                         1.791667\n",
            "Cultural and Contextual Understanding    3.100000\n",
            "Emotion Analysis                         4.200000\n",
            "Ethics Analysis                          2.000000\n",
            "Fact Analysis                            3.875000\n",
            "Logical Reasoning                        2.909091\n",
            "Reading Comprehension                    2.333333\n",
            "Reframing                                2.250000\n",
            "Safety and Security                      4.875000\n",
            "Summarization                            3.333333\n",
            "Name: quality, dtype: float64\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ2cjVDUhdHqY5Lg4O333U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}